// automatically generated by the FlatBuffers compiler, do not modify


#ifndef FLATBUFFERS_GENERATED_CUDAENGINE_PPL_NN_PMX_CUDA_H_
#define FLATBUFFERS_GENERATED_CUDAENGINE_PPL_NN_PMX_CUDA_H_

#include "flatbuffers/flatbuffers.h"

// Ensure the included flatbuffers.h is the same version as when this file was
// generated, otherwise it may not be compatible.
static_assert(FLATBUFFERS_VERSION_MAJOR == 2 &&
              FLATBUFFERS_VERSION_MINOR == 0 &&
              FLATBUFFERS_VERSION_REVISION == 8,
             "Non-compatible flatbuffers version included");

namespace ppl {
namespace nn {
namespace pmx {
namespace cuda {

struct CudaTensorQuant;
struct CudaTensorQuantBuilder;

struct TensorQuants;
struct TensorQuantsBuilder;

struct CudaEngineParam;
struct CudaEngineParamBuilder;

enum CudaEngineParamType : uint8_t {
  CudaEngineParamType_NONE = 0,
  CudaEngineParamType_TensorQuants = 1,
  CudaEngineParamType_MIN = CudaEngineParamType_NONE,
  CudaEngineParamType_MAX = CudaEngineParamType_TensorQuants
};

inline const CudaEngineParamType (&EnumValuesCudaEngineParamType())[2] {
  static const CudaEngineParamType values[] = {
    CudaEngineParamType_NONE,
    CudaEngineParamType_TensorQuants
  };
  return values;
}

inline const char * const *EnumNamesCudaEngineParamType() {
  static const char * const names[3] = {
    "NONE",
    "TensorQuants",
    nullptr
  };
  return names;
}

inline const char *EnumNameCudaEngineParamType(CudaEngineParamType e) {
  if (flatbuffers::IsOutRange(e, CudaEngineParamType_NONE, CudaEngineParamType_TensorQuants)) return "";
  const size_t index = static_cast<size_t>(e);
  return EnumNamesCudaEngineParamType()[index];
}

template<typename T> struct CudaEngineParamTypeTraits {
  static const CudaEngineParamType enum_value = CudaEngineParamType_NONE;
};

template<> struct CudaEngineParamTypeTraits<ppl::nn::pmx::cuda::TensorQuants> {
  static const CudaEngineParamType enum_value = CudaEngineParamType_TensorQuants;
};

bool VerifyCudaEngineParamType(flatbuffers::Verifier &verifier, const void *obj, CudaEngineParamType type);
bool VerifyCudaEngineParamTypeVector(flatbuffers::Verifier &verifier, const flatbuffers::Vector<flatbuffers::Offset<void>> *values, const flatbuffers::Vector<uint8_t> *types);

struct CudaTensorQuant FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
  typedef CudaTensorQuantBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_FORMAT = 4,
    VT_TYPE = 6,
    VT_PER_CHANNEL = 8,
    VT_BIT_WIDTH = 10,
    VT_SCALE = 12,
    VT_ZERO_POINT = 14
  };
  int32_t format() const {
    return GetField<int32_t>(VT_FORMAT, 0);
  }
  int32_t type() const {
    return GetField<int32_t>(VT_TYPE, 0);
  }
  bool per_channel() const {
    return GetField<uint8_t>(VT_PER_CHANNEL, 0) != 0;
  }
  int32_t bit_width() const {
    return GetField<int32_t>(VT_BIT_WIDTH, 0);
  }
  const flatbuffers::Vector<float> *scale() const {
    return GetPointer<const flatbuffers::Vector<float> *>(VT_SCALE);
  }
  const flatbuffers::Vector<float> *zero_point() const {
    return GetPointer<const flatbuffers::Vector<float> *>(VT_ZERO_POINT);
  }
  bool Verify(flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<int32_t>(verifier, VT_FORMAT, 4) &&
           VerifyField<int32_t>(verifier, VT_TYPE, 4) &&
           VerifyField<uint8_t>(verifier, VT_PER_CHANNEL, 1) &&
           VerifyField<int32_t>(verifier, VT_BIT_WIDTH, 4) &&
           VerifyOffset(verifier, VT_SCALE) &&
           verifier.VerifyVector(scale()) &&
           VerifyOffset(verifier, VT_ZERO_POINT) &&
           verifier.VerifyVector(zero_point()) &&
           verifier.EndTable();
  }
};

struct CudaTensorQuantBuilder {
  typedef CudaTensorQuant Table;
  flatbuffers::FlatBufferBuilder &fbb_;
  flatbuffers::uoffset_t start_;
  void add_format(int32_t format) {
    fbb_.AddElement<int32_t>(CudaTensorQuant::VT_FORMAT, format, 0);
  }
  void add_type(int32_t type) {
    fbb_.AddElement<int32_t>(CudaTensorQuant::VT_TYPE, type, 0);
  }
  void add_per_channel(bool per_channel) {
    fbb_.AddElement<uint8_t>(CudaTensorQuant::VT_PER_CHANNEL, static_cast<uint8_t>(per_channel), 0);
  }
  void add_bit_width(int32_t bit_width) {
    fbb_.AddElement<int32_t>(CudaTensorQuant::VT_BIT_WIDTH, bit_width, 0);
  }
  void add_scale(flatbuffers::Offset<flatbuffers::Vector<float>> scale) {
    fbb_.AddOffset(CudaTensorQuant::VT_SCALE, scale);
  }
  void add_zero_point(flatbuffers::Offset<flatbuffers::Vector<float>> zero_point) {
    fbb_.AddOffset(CudaTensorQuant::VT_ZERO_POINT, zero_point);
  }
  explicit CudaTensorQuantBuilder(flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  flatbuffers::Offset<CudaTensorQuant> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = flatbuffers::Offset<CudaTensorQuant>(end);
    return o;
  }
};

inline flatbuffers::Offset<CudaTensorQuant> CreateCudaTensorQuant(
    flatbuffers::FlatBufferBuilder &_fbb,
    int32_t format = 0,
    int32_t type = 0,
    bool per_channel = false,
    int32_t bit_width = 0,
    flatbuffers::Offset<flatbuffers::Vector<float>> scale = 0,
    flatbuffers::Offset<flatbuffers::Vector<float>> zero_point = 0) {
  CudaTensorQuantBuilder builder_(_fbb);
  builder_.add_zero_point(zero_point);
  builder_.add_scale(scale);
  builder_.add_bit_width(bit_width);
  builder_.add_type(type);
  builder_.add_format(format);
  builder_.add_per_channel(per_channel);
  return builder_.Finish();
}

inline flatbuffers::Offset<CudaTensorQuant> CreateCudaTensorQuantDirect(
    flatbuffers::FlatBufferBuilder &_fbb,
    int32_t format = 0,
    int32_t type = 0,
    bool per_channel = false,
    int32_t bit_width = 0,
    const std::vector<float> *scale = nullptr,
    const std::vector<float> *zero_point = nullptr) {
  auto scale__ = scale ? _fbb.CreateVector<float>(*scale) : 0;
  auto zero_point__ = zero_point ? _fbb.CreateVector<float>(*zero_point) : 0;
  return ppl::nn::pmx::cuda::CreateCudaTensorQuant(
      _fbb,
      format,
      type,
      per_channel,
      bit_width,
      scale__,
      zero_point__);
}

struct TensorQuants FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
  typedef TensorQuantsBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_TENSOR_QUANTS = 4
  };
  const flatbuffers::Vector<flatbuffers::Offset<ppl::nn::pmx::cuda::CudaTensorQuant>> *tensor_quants() const {
    return GetPointer<const flatbuffers::Vector<flatbuffers::Offset<ppl::nn::pmx::cuda::CudaTensorQuant>> *>(VT_TENSOR_QUANTS);
  }
  bool Verify(flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_TENSOR_QUANTS) &&
           verifier.VerifyVector(tensor_quants()) &&
           verifier.VerifyVectorOfTables(tensor_quants()) &&
           verifier.EndTable();
  }
};

struct TensorQuantsBuilder {
  typedef TensorQuants Table;
  flatbuffers::FlatBufferBuilder &fbb_;
  flatbuffers::uoffset_t start_;
  void add_tensor_quants(flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<ppl::nn::pmx::cuda::CudaTensorQuant>>> tensor_quants) {
    fbb_.AddOffset(TensorQuants::VT_TENSOR_QUANTS, tensor_quants);
  }
  explicit TensorQuantsBuilder(flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  flatbuffers::Offset<TensorQuants> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = flatbuffers::Offset<TensorQuants>(end);
    return o;
  }
};

inline flatbuffers::Offset<TensorQuants> CreateTensorQuants(
    flatbuffers::FlatBufferBuilder &_fbb,
    flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<ppl::nn::pmx::cuda::CudaTensorQuant>>> tensor_quants = 0) {
  TensorQuantsBuilder builder_(_fbb);
  builder_.add_tensor_quants(tensor_quants);
  return builder_.Finish();
}

inline flatbuffers::Offset<TensorQuants> CreateTensorQuantsDirect(
    flatbuffers::FlatBufferBuilder &_fbb,
    const std::vector<flatbuffers::Offset<ppl::nn::pmx::cuda::CudaTensorQuant>> *tensor_quants = nullptr) {
  auto tensor_quants__ = tensor_quants ? _fbb.CreateVector<flatbuffers::Offset<ppl::nn::pmx::cuda::CudaTensorQuant>>(*tensor_quants) : 0;
  return ppl::nn::pmx::cuda::CreateTensorQuants(
      _fbb,
      tensor_quants__);
}

struct CudaEngineParam FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
  typedef CudaEngineParamBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_VALUE_TYPE = 4,
    VT_VALUE = 6
  };
  ppl::nn::pmx::cuda::CudaEngineParamType value_type() const {
    return static_cast<ppl::nn::pmx::cuda::CudaEngineParamType>(GetField<uint8_t>(VT_VALUE_TYPE, 0));
  }
  const void *value() const {
    return GetPointer<const void *>(VT_VALUE);
  }
  template<typename T> const T *value_as() const;
  const ppl::nn::pmx::cuda::TensorQuants *value_as_TensorQuants() const {
    return value_type() == ppl::nn::pmx::cuda::CudaEngineParamType_TensorQuants ? static_cast<const ppl::nn::pmx::cuda::TensorQuants *>(value()) : nullptr;
  }
  bool Verify(flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<uint8_t>(verifier, VT_VALUE_TYPE, 1) &&
           VerifyOffset(verifier, VT_VALUE) &&
           VerifyCudaEngineParamType(verifier, value(), value_type()) &&
           verifier.EndTable();
  }
};

template<> inline const ppl::nn::pmx::cuda::TensorQuants *CudaEngineParam::value_as<ppl::nn::pmx::cuda::TensorQuants>() const {
  return value_as_TensorQuants();
}

struct CudaEngineParamBuilder {
  typedef CudaEngineParam Table;
  flatbuffers::FlatBufferBuilder &fbb_;
  flatbuffers::uoffset_t start_;
  void add_value_type(ppl::nn::pmx::cuda::CudaEngineParamType value_type) {
    fbb_.AddElement<uint8_t>(CudaEngineParam::VT_VALUE_TYPE, static_cast<uint8_t>(value_type), 0);
  }
  void add_value(flatbuffers::Offset<void> value) {
    fbb_.AddOffset(CudaEngineParam::VT_VALUE, value);
  }
  explicit CudaEngineParamBuilder(flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  flatbuffers::Offset<CudaEngineParam> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = flatbuffers::Offset<CudaEngineParam>(end);
    return o;
  }
};

inline flatbuffers::Offset<CudaEngineParam> CreateCudaEngineParam(
    flatbuffers::FlatBufferBuilder &_fbb,
    ppl::nn::pmx::cuda::CudaEngineParamType value_type = ppl::nn::pmx::cuda::CudaEngineParamType_NONE,
    flatbuffers::Offset<void> value = 0) {
  CudaEngineParamBuilder builder_(_fbb);
  builder_.add_value(value);
  builder_.add_value_type(value_type);
  return builder_.Finish();
}

inline bool VerifyCudaEngineParamType(flatbuffers::Verifier &verifier, const void *obj, CudaEngineParamType type) {
  switch (type) {
    case CudaEngineParamType_NONE: {
      return true;
    }
    case CudaEngineParamType_TensorQuants: {
      auto ptr = reinterpret_cast<const ppl::nn::pmx::cuda::TensorQuants *>(obj);
      return verifier.VerifyTable(ptr);
    }
    default: return true;
  }
}

inline bool VerifyCudaEngineParamTypeVector(flatbuffers::Verifier &verifier, const flatbuffers::Vector<flatbuffers::Offset<void>> *values, const flatbuffers::Vector<uint8_t> *types) {
  if (!values || !types) return !values && !types;
  if (values->size() != types->size()) return false;
  for (flatbuffers::uoffset_t i = 0; i < values->size(); ++i) {
    if (!VerifyCudaEngineParamType(
        verifier,  values->Get(i), types->GetEnum<CudaEngineParamType>(i))) {
      return false;
    }
  }
  return true;
}

inline const ppl::nn::pmx::cuda::CudaEngineParam *GetCudaEngineParam(const void *buf) {
  return flatbuffers::GetRoot<ppl::nn::pmx::cuda::CudaEngineParam>(buf);
}

inline const ppl::nn::pmx::cuda::CudaEngineParam *GetSizePrefixedCudaEngineParam(const void *buf) {
  return flatbuffers::GetSizePrefixedRoot<ppl::nn::pmx::cuda::CudaEngineParam>(buf);
}

inline bool VerifyCudaEngineParamBuffer(
    flatbuffers::Verifier &verifier) {
  return verifier.VerifyBuffer<ppl::nn::pmx::cuda::CudaEngineParam>(nullptr);
}

inline bool VerifySizePrefixedCudaEngineParamBuffer(
    flatbuffers::Verifier &verifier) {
  return verifier.VerifySizePrefixedBuffer<ppl::nn::pmx::cuda::CudaEngineParam>(nullptr);
}

inline void FinishCudaEngineParamBuffer(
    flatbuffers::FlatBufferBuilder &fbb,
    flatbuffers::Offset<ppl::nn::pmx::cuda::CudaEngineParam> root) {
  fbb.Finish(root);
}

inline void FinishSizePrefixedCudaEngineParamBuffer(
    flatbuffers::FlatBufferBuilder &fbb,
    flatbuffers::Offset<ppl::nn::pmx::cuda::CudaEngineParam> root) {
  fbb.FinishSizePrefixed(root);
}

}  // namespace cuda
}  // namespace pmx
}  // namespace nn
}  // namespace ppl

#endif  // FLATBUFFERS_GENERATED_CUDAENGINE_PPL_NN_PMX_CUDA_H_
